apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: SERVICE_ID_PLACEHOLDER
  namespace: default
  labels:
    yunikorn.apache.org/app-id: "SERVICE_ID_PLACEHOLDER"
    build-number: "BUILD_NUMBER"
    spark-app: "true"
spec:
  type: Scala
  mode: cluster
  image: docker.io/library/spark:BUILD_NUMBER
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples.jar
  sparkVersion: 4.0.1
  driver:
    cores: 1
    memory: 512m
    podName: "SERVICE_ID_PLACEHOLDER"
    labels:
      yunikorn.apache.org/app-id: "SERVICE_ID_PLACEHOLDER"
      build-number: "BUILD_NUMBER"
      spark-app: "true"
    annotations:
      # 1. Driver 자신이 속할 그룹 이름
      yunikorn.apache.org/task-group-name: "spark-driver"
      # Prometheus scraping annotations for driver
      prometheus.io/scrape: "true"
      prometheus.io/port: "4040"
      prometheus.io/path: "/metrics/driver/prometheus/"
      yunikorn.apache.org/task-groups: |-
        [
          {
            "name": "spark-driver",
            "minMember": 1,
            "minResource": {
              "cpu": "100m",
              "memory": "512Mi"
            },
            "nodeSelector": {},
            "tolerations": []
          },
          {
            "name": "spark-executor",
            "minMember": 1,
            "minResource": {
              "cpu": "100m",
              "memory": "512Mi"
            },
            "nodeSelector": {},
            "tolerations": []
          }
        ]
    serviceAccount: spark-operator-spark
    template:
      spec:
        containers:
          - name: SERVICE_ID_PLACEHOLDER
            image: docker.io/library/spark:BUILD_NUMBER
            resources:
              limits:
                memory: 512m
                cpu: "1"
              requests:
                memory: 512m
                cpu: "500m"
    securityContext:
      capabilities:
        drop:
        - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
  executor:
    instances: 2
    cores: 1
    memory: 512m
    labels:
      yunikorn.apache.org/app-id: "SERVICE_ID_PLACEHOLDER"
      build-number: "BUILD_NUMBER"
      spark-app: "true"
    annotations:
      # Executor가 속한 그룹 이름 지정
      yunikorn.apache.org/task-group-name: "spark-executor"
      # Prometheus scraping annotations for executor
      prometheus.io/scrape: "true"
      prometheus.io/port: "4040"
      prometheus.io/path: "/metrics/executors/prometheus/"
    template:
      spec:
        containers:
          - name: SERVICE_ID_PLACEHOLDER
            image: docker.io/library/spark:BUILD_NUMBER
            resources:
              limits:
                memory: 512m
                cpu: "1"
              requests:
                memory: 512m
                cpu: "500m"
    securityContext:
      capabilities:
        drop:
        - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
  sparkConf:
    spark.app.name: "SERVICE_ID_PLACEHOLDER"
    spark.kubernetes.executor.podNamePrefix: "SERVICE_ID_PLACEHOLDER"
    spark.kubernetes.driver.pod.name: "SERVICE_ID_PLACEHOLDER"
    spark.sql.shuffle.partitions: "100"
    spark.default.parallelism: "200"
    spark.ui.retainedStages: "50"
    spark.ui.retainedJobs: "50"
    spark.ui.retainedTasks: "1000"
    # Disable dynamic allocation to keep executors alive longer
    spark.dynamicAllocation.enabled: "false"
    spark.dynamicAllocation.shuffleTracking.enabled: "false"
    spark.kubernetes.executor.deleteOnTermination: "false"
    spark.kubernetes.driver.deleteOnTermination: "false"
    # Prometheus metrics configuration
    spark.kubernetes.driver.annotation.prometheus.io/scrape: "true"
    spark.kubernetes.driver.annotation.prometheus.io/path: "/metrics/driver/prometheus/"
    spark.kubernetes.driver.annotation.prometheus.io/port: "4040"
    spark.kubernetes.driver.service.annotation.prometheus.io/scrape: "true"
    spark.kubernetes.driver.service.annotation.prometheus.io/path: "/metrics/driver/prometheus/"
    spark.kubernetes.driver.service.annotation.prometheus.io/port: "4040"
    spark.ui.prometheus.enabled: "true"
    spark.executor.processTreeMetrics.enabled: "true"
    spark.metrics.conf.*.sink.prometheusServlet.class: "org.apache.spark.metrics.sink.PrometheusServlet"
    spark.metrics.conf.driver.sink.prometheusServlet.path: "/metrics/driver/prometheus/"
    spark.metrics.conf.executor.sink.prometheusServlet.path: "/metrics/executors/prometheus/" 
  # SparkApplication 객체 종료 후 2시간(7200초) 동안 유지
  # 참고: v1beta2 API에서는 파드 보존(cleanPodPolicy) 필드가 없음
  timeToLiveSeconds: 7200
  # Large number for longer runtime (1M iterations)
  arguments:
    - "1000000"

  batchScheduler: yunikorn
  batchSchedulerOptions:
    queue: root.default
